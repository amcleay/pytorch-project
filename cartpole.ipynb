{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 60000\n",
    "total = 0\n",
    "total_reward = 0\n",
    "prior_reward = 0\n",
    "\n",
    "Observation = [30, 30, 50, 50]\n",
    "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "\n",
    "epsilon = 1\n",
    "\n",
    "epsilon_decay_value = 0.99995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30, 50, 50, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    discrete_state = state/np_array_win_size+ np.array([15,10,1,10])\n",
    "    return tuple(discrete_state.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9w/m53zq1sx3c95nc1tzpn5m9ww0000gn/T/ipykernel_21075/2102410102.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return tuple(discrete_state.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Time Average: 0.000754969835281372\n",
      "Mean Reward: 0.044\n",
      "Time Average: 0.0008153102397918701\n",
      "Mean Reward: 21.394\n",
      "Episode: 2000\n",
      "Time Average: 0.001114626407623291\n",
      "Mean Reward: 22.178\n",
      "Time Average: 0.0008175504207611084\n",
      "Mean Reward: 21.503\n",
      "Episode: 4000\n",
      "Time Average: 0.0009635334014892578\n",
      "Mean Reward: 22.073\n",
      "Time Average: 0.0008460764884948731\n",
      "Mean Reward: 22.268\n",
      "Episode: 6000\n",
      "Time Average: 0.0013848707675933839\n",
      "Mean Reward: 22.065\n",
      "Time Average: 0.0008254778385162353\n",
      "Mean Reward: 22.374\n",
      "Episode: 8000\n",
      "Time Average: 0.0011897406578063965\n",
      "Mean Reward: 22.281\n",
      "Time Average: 0.0008478891849517822\n",
      "Mean Reward: 22.454\n",
      "Episode: 10000\n",
      "Time Average: 0.0012664740085601806\n",
      "Mean Reward: 22.556\n",
      "Time Average: 0.0008303835391998291\n",
      "Mean Reward: 22.241\n",
      "Episode: 12000\n",
      "Time Average: 0.0011176402568817139\n",
      "Mean Reward: 23.993\n",
      "Epsilon: 0.8824941446941661\n",
      "Epsilon: 0.8607047486686201\n",
      "Time Average: 0.0010014488697052003\n",
      "Mean Reward: 24.815\n",
      "Episode: 14000\n",
      "Epsilon: 0.818726659298009\n",
      "Time Average: 0.001512570858001709\n",
      "Mean Reward: 27.118\n",
      "Epsilon: 0.7787959154194878\n",
      "Time Average: 0.001121945858001709\n",
      "Mean Reward: 28.649\n",
      "Epsilon: 0.7595669010105212\n",
      "Episode: 16000\n",
      "Epsilon: 0.7408126643807126\n",
      "Time Average: 0.001458449602127075\n",
      "Mean Reward: 32.114\n",
      "Epsilon: 0.7225214829355084\n",
      "Time Average: 0.001242358684539795\n",
      "Mean Reward: 33.893\n",
      "Episode: 18000\n",
      "Epsilon: 0.6703133426452782\n",
      "Time Average: 0.0019408886432647704\n",
      "Mean Reward: 36.6\n",
      "Time Average: 0.0014384496212005615\n",
      "Mean Reward: 39.289\n",
      "Epsilon: 0.6218776713776856\n",
      "Episode: 20000\n",
      "Epsilon: 0.606523077874078\n",
      "Time Average: 0.0023570892810821534\n",
      "Mean Reward: 43.842\n",
      "Epsilon: 0.5769418771107269\n",
      "Time Average: 0.001721785306930542\n",
      "Mean Reward: 47.594\n",
      "Episode: 22000\n",
      "Epsilon: 0.5488034037068503\n",
      "Time Average: 0.00307611608505249\n",
      "Mean Reward: 50.791\n",
      "Epsilon: 0.5352530648457575\n",
      "Epsilon: 0.5220372933033263\n",
      "Time Average: 0.002014822006225586\n",
      "Mean Reward: 54.804\n",
      "Episode: 24000\n",
      "Time Average: 0.0024498140811920165\n",
      "Mean Reward: 60.121\n",
      "Epsilon: 0.484315790359524\n",
      "Time Average: 0.0024307591915130617\n",
      "Mean Reward: 63.638\n",
      "Episode: 26000\n",
      "Time Average: 0.0029835541248321535\n",
      "Mean Reward: 69.528\n",
      "Time Average: 0.0027164669036865235\n",
      "Mean Reward: 74.998\n",
      "Epsilon: 0.41685290061763824\n",
      "Episode: 28000\n",
      "Time Average: 0.003697457551956177\n",
      "Mean Reward: 82.588\n",
      "Time Average: 0.003065706968307495\n",
      "Mean Reward: 86.268\n",
      "Episode: 30000\n",
      "Epsilon: 0.3678702439938449\n",
      "Time Average: 0.003765458106994629\n",
      "Mean Reward: 87.659\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [13]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     17\u001B[0m     action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m, env\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mn) \u001B[38;5;66;03m#do a random ation\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m new_state, reward, done, _ \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#step action to get new states, reward, and the \"done\" status.\u001B[39;00m\n\u001B[1;32m     21\u001B[0m episode_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward \u001B[38;5;66;03m#add the reward\u001B[39;00m\n\u001B[1;32m     23\u001B[0m new_discrete_state \u001B[38;5;241m=\u001B[39m get_discrete_state(new_state)\n",
      "File \u001B[0;32m~/PycharmProjects/hacking/venv/lib/python3.9/site-packages/gym/wrappers/time_limit.py:18\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[1;32m     16\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     17\u001B[0m     ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 18\u001B[0m     observation, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[0;32m~/PycharmProjects/hacking/venv/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py:110\u001B[0m, in \u001B[0;36mCartPoleEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    108\u001B[0m force \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforce_mag \u001B[38;5;28;01mif\u001B[39;00m action \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforce_mag\n\u001B[1;32m    109\u001B[0m costheta \u001B[38;5;241m=\u001B[39m math\u001B[38;5;241m.\u001B[39mcos(theta)\n\u001B[0;32m--> 110\u001B[0m sintheta \u001B[38;5;241m=\u001B[39m \u001B[43mmath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtheta\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;66;03m# For the interested reader:\u001B[39;00m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m# https://coneural.org/florian/papers/05_cart_pole.pdf\u001B[39;00m\n\u001B[1;32m    114\u001B[0m temp \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    115\u001B[0m     force \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolemass_length \u001B[38;5;241m*\u001B[39m theta_dot \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m sintheta\n\u001B[1;32m    116\u001B[0m ) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal_mass\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(EPISODES + 1): #go through the episodes\n",
    "    t0 = time.time() #set the initial time\n",
    "    discrete_state = get_discrete_state(env.reset()) #get the discrete start for the restarted environment \n",
    "    done = False\n",
    "    episode_reward = 0 #reward starts as 0 for each episode\n",
    "\n",
    "    if episode % 2000 == 0: \n",
    "        print(\"Episode: \" + str(episode))\n",
    "\n",
    "    while not done: \n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "\n",
    "            action = np.argmax(q_table[discrete_state]) #take cordinated action\n",
    "        else:\n",
    "\n",
    "            action = np.random.randint(0, env.action_space.n) #do a random ation\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action) #step action to get new states, reward, and the \"done\" status.\n",
    "\n",
    "        episode_reward += reward #add the reward\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if episode % 2000 == 0: #render\n",
    "            env.render()\n",
    "\n",
    "        if not done: #update q-table\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    if epsilon > 0.05: #epsilon modification\n",
    "        if episode_reward > prior_reward and episode > 10000:\n",
    "            epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
    "\n",
    "            if episode % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "\n",
    "    t1 = time.time() #episode has finished\n",
    "    episode_total = t1 - t0 #episode total time\n",
    "    total = total + episode_total\n",
    "\n",
    "    total_reward += episode_reward #episode total reward\n",
    "    prior_reward = episode_reward\n",
    "\n",
    "    if episode % 1000 == 0: #every 1000 episodes print the average time and the average reward\n",
    "        mean = total / 1000\n",
    "        print(\"Time Average: \" + str(mean))\n",
    "        total = 0\n",
    "\n",
    "        mean_reward = total_reward / 1000\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        total_reward = 0\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}